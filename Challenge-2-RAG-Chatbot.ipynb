{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ewuCmofJhgKW",
      "metadata": {
        "id": "ewuCmofJhgKW"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from google import genai\n",
        "from google.colab import auth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A6ql5TwYhWSu",
      "metadata": {
        "id": "A6ql5TwYhWSu"
      },
      "source": [
        "### Initialize Clients\n",
        "\n",
        "Sets up the required clients to interact with Google Cloud services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cTh6esn4gASKLC2NxFF8bTBP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTh6esn4gASKLC2NxFF8bTBP",
        "outputId": "9954b361-0d55-43ba-c6bb-4abe1f8cf2f0",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigquery and Gemini clients Initialized successfully\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-03-be6b72ae4b0d\"\n",
        "\n",
        "#Big Query client\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Create Gemini client using Vertex AI\n",
        "gemini_client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=\"qwiklabs-gcp-03-be6b72ae4b0d\",\n",
        "    location=\"global\"\n",
        ")\n",
        "\n",
        "print(\"Bigquery and Gemini clients Initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80vzdVXMAKSJ",
      "metadata": {
        "id": "80vzdVXMAKSJ"
      },
      "source": [
        "### Load Aurora Bay FAQ Data\n",
        "\n",
        "Create a BigQuery dataset (`aurora_bay_rag`) and loads a CSV file containing FAQs from a public GCS bucket into a table named `faq_data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "YPPt59o5jtt0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPPt59o5jtt0",
        "outputId": "9fb9b93e-a7c3-41dc-9c35-ec9b23532809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully into: qwiklabs-gcp-03-be6b72ae4b0d.aurora_bay_rag.faq_data\n"
          ]
        }
      ],
      "source": [
        "# Load Aurora Bay FAQs from GCS to BigQuery\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Define dataset and table\n",
        "dataset_id = \"aurora_bay_rag\"\n",
        "table_id = \"faq_data\"\n",
        "table_ref = f\"{PROJECT_ID}.{dataset_id}.{table_id}\"\n",
        "\n",
        "# Create the dataset (if it doesn't exist)\n",
        "dataset_ref = bigquery.Dataset(f\"{PROJECT_ID}.{dataset_id}\")\n",
        "dataset_ref.location = \"US\"\n",
        "bq_client.create_dataset(dataset_ref, exists_ok=True)\n",
        "\n",
        "# CSV source in GCS\n",
        "gcs_uri = \"gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv\"\n",
        "\n",
        "# Load configuration\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    autodetect=True,\n",
        "    skip_leading_rows=1,\n",
        "    source_format=bigquery.SourceFormat.CSV\n",
        ")\n",
        "\n",
        "# Load job\n",
        "load_job = bq_client.load_table_from_uri(\n",
        "    gcs_uri,\n",
        "    table_ref,\n",
        "    job_config=job_config\n",
        ")\n",
        "\n",
        "load_job.result()\n",
        "\n",
        "print(f\"Data loaded successfully into: {table_ref}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q21yZSbFAgld",
      "metadata": {
        "id": "Q21yZSbFAgld"
      },
      "source": [
        "### Create remote embedding model using BigQuery ML\n",
        "Note: Before running the query, make sure to,\n",
        "1. Create External Connection named embedding_conn in Big Query Studio\n",
        "2. Grant the service account the necessary IAM permissions (BigQuery Data Owner and Vertex AI User)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "x2TZqMF4l5S0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2TZqMF4l5S0",
        "outputId": "573cf5ca-dad8-44c6-f6e7-5b5a8d95f81b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding model created: aurora_bay_rag.faq_embedding_model\n"
          ]
        }
      ],
      "source": [
        "embedding_model_query = \"\"\"\n",
        "CREATE OR REPLACE MODEL `aurora_bay_rag.faq_embedding_model`\n",
        "REMOTE WITH CONNECTION `us.embedding_conn`\n",
        "OPTIONS (ENDPOINT = 'text-embedding-005');\n",
        "\"\"\"\n",
        "\n",
        "job = bq_client.query(embedding_model_query)\n",
        "job.result()\n",
        "\n",
        "print(\"Embedding model created: aurora_bay_rag.faq_embedding_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qBm3XBXHAsk7",
      "metadata": {
        "id": "qBm3XBXHAsk7"
      },
      "source": [
        "### (Optional) Preview the column and first few rows of the table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Y8MoUY-dsxcz",
      "metadata": {
        "id": "Y8MoUY-dsxcz"
      },
      "outputs": [],
      "source": [
        "table = bq_client.get_table(\"aurora_bay_rag.faq_data\")\n",
        "\n",
        "preview_df = bq_client.query(\"\"\"\n",
        "SELECT * FROM `aurora_bay_rag.faq_embedded`\n",
        "LIMIT 5\n",
        "\"\"\").to_dataframe()\n",
        "\n",
        "# preview_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2pPTUfBCA2-i",
      "metadata": {
        "id": "2pPTUfBCA2-i"
      },
      "source": [
        "### Generate embeddings and store them in a new table\n",
        "\n",
        "Generate the embedding from the `faq_data` table to new `faq_embedded` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "R9u212d8l9sC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9u212d8l9sC",
        "outputId": "791122c0-d530-48a0-ff8e-abc9d4fb673f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings generated and stored in: aurora_bay_rag.faq_embedded\n"
          ]
        }
      ],
      "source": [
        "generate_embed_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE `aurora_bay_rag.faq_embedded` AS\n",
        "SELECT *,\n",
        "       ml_generate_embedding_result AS embedding\n",
        "FROM ML.GENERATE_EMBEDDING(\n",
        "  MODEL `aurora_bay_rag.faq_embedding_model`,\n",
        "  (\n",
        "    SELECT\n",
        "      CONCAT(string_field_0, ' ', string_field_1) AS content,\n",
        "      string_field_0 AS question,\n",
        "      string_field_1 AS answer\n",
        "    FROM `aurora_bay_rag.faq_data`\n",
        "  )\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "bq_client.query(generate_embed_sql).result()\n",
        "\n",
        "print(\"Embeddings generated and stored in: aurora_bay_rag.faq_embedded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pm3_oe57BIb6",
      "metadata": {
        "id": "Pm3_oe57BIb6"
      },
      "source": [
        "### (Optional) Preview contents of the Embeddings table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "h2DoA5PxyD5_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2DoA5PxyD5_",
        "outputId": "1452e269-5011-44e5-854d-3e3ef4a2566b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in aurora_bay_rag.faq_embedded:\n",
            "- ml_generate_embedding_result (FLOAT)\n",
            "- ml_generate_embedding_statistics (JSON)\n",
            "- ml_generate_embedding_status (STRING)\n",
            "- content (STRING)\n",
            "- question (STRING)\n",
            "- answer (STRING)\n",
            "- embedding (FLOAT)\n"
          ]
        }
      ],
      "source": [
        "# Run only if you need to view the columns in embedding table\n",
        "table = bq_client.get_table(\"aurora_bay_rag.faq_embedded\")\n",
        "print(\"Columns in aurora_bay_rag.faq_embedded:\")\n",
        "for field in table.schema:\n",
        "    print(f\"- {field.name} ({field.field_type})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NT3WdaB5BZAP",
      "metadata": {
        "id": "NT3WdaB5BZAP"
      },
      "source": [
        "### Perform a Vector Search for the user query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "QbwlOSyI0X6v",
      "metadata": {
        "id": "QbwlOSyI0X6v"
      },
      "outputs": [],
      "source": [
        "def search_faq_results(user_question: str, top_k: int = 3):\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "      query.query,\n",
        "      result.base.question,\n",
        "      result.base.answer,\n",
        "      result.distance\n",
        "    FROM VECTOR_SEARCH(\n",
        "      TABLE `aurora_bay_rag.faq_embedded`,\n",
        "      'embedding',\n",
        "      (\n",
        "        SELECT\n",
        "          ml_generate_embedding_result AS embedding,\n",
        "          '{user_question}' AS query\n",
        "        FROM ML.GENERATE_EMBEDDING(\n",
        "          MODEL `aurora_bay_rag.faq_embedding_model`,\n",
        "          (SELECT '{user_question}' AS content)\n",
        "        )\n",
        "      ),\n",
        "      top_k => {top_k},\n",
        "      options => '{{\"fraction_lists_to_search\": 1.0}}'\n",
        "    ) AS result\n",
        "    \"\"\"\n",
        "    return bq_client.query(query).to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "buU6jG-TDuRY",
      "metadata": {
        "id": "buU6jG-TDuRY"
      },
      "source": [
        "### Generate Answer from gemini with the FAQ context\n",
        "\n",
        "This function takes the user’s question and a list of similar FAQs (from the vector search),\n",
        "and constructs a prompt for the Gemini.\n",
        "\n",
        "Gemini uses the FAQ context to generate a friendly and helpful response.\n",
        "It ensures the assistant stays relevant to Aurora Bay and gives natural-language answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "J-WH2WAPwK5R",
      "metadata": {
        "id": "J-WH2WAPwK5R"
      },
      "outputs": [],
      "source": [
        "def generate_answer_from_gemini(user_question: str, faq_context_df):\n",
        "    # Format the FAQ context for Gemini\n",
        "    context_text = \"\\n\".join([\n",
        "        f\"Q: {row['question']}\\nA: {row['answer']}\" for _, row in faq_context_df.iterrows()\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful assistant for the town of Aurora Bay, Alaska.\n",
        "Use the frequently asked questions below to answer the user question. Do not answer questions that are not in the context.\n",
        "Say I am unable to answer the question if you do not know the answer.\n",
        "\n",
        "FAQs:\n",
        "{context_text}\n",
        "\n",
        "User question: {user_question}\n",
        "\n",
        "Answer in a clear, concise, and friendly tone:\n",
        "\"\"\"\n",
        "\n",
        "    contents = [\n",
        "        genai.types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[genai.types.Part(text=prompt)]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    response = gemini_client.models.generate_content(\n",
        "        model=\"gemini-2.5-pro-preview-06-05\",\n",
        "        contents=contents,\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            temperature=0.7,\n",
        "            max_output_tokens=1024\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if response.candidates and response.candidates[0].content.parts:\n",
        "        return response.candidates[0].content.parts[0].text.strip()\n",
        "    else:\n",
        "        return \"Oops! Error fetching response\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DU5do2VFD1_q",
      "metadata": {
        "id": "DU5do2VFD1_q"
      },
      "source": [
        "### Chatbot Interface (Optional to use)\n",
        "\n",
        "Just call the `generate_answer_from_gemini(\"Where can I pay my water bill?\")` function with your query to get the answers from RAG.\n",
        "\n",
        "Optionally use the below interface to get a conversational chatbot like experience.\n",
        "\n",
        "This function starts a text-based chatbot loop. The assistant:\n",
        "- Greets the user\n",
        "- Accepts their question\n",
        "- Uses vector search + Gemini to generate a response\n",
        "- Asks if the user wants to see FAQ sources or ask another question\n",
        "- Lets the user exit any time by typing 'exit'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kf5d0DzNGhbT",
      "metadata": {
        "id": "Kf5d0DzNGhbT"
      },
      "outputs": [],
      "source": [
        "generate_answer_from_gemini(\"Where can I pay my water bill?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8frP-FYq9s2M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8frP-FYq9s2M",
        "outputId": "a3bcd937-1d5e-4b53-9a41-1b45307c2ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi! I’m an assistant for Aurora Bay, Alaska.\n",
            "\n",
            "What would you like to know? (Type 'exit / end' to end)\n",
            "\n",
            "You: Where can I pay my water bill?\n",
            "\n",
            " Gemini: Hello! You can pay your water bill online, by mail, or in person at the Aurora Bay Utilities Department located within Town Hall.\n",
            "\n",
            "Would you like to view the sources? Type 'yes' to see FAQs, 'next' to ask another question, or 'exit / end' to quit: yes\n",
            "\n",
            "Top Matching FAQs:\n",
            "\n",
            "Q1: Where can I pay my water and sewer bills?\n",
            "A1: You can pay water and sewer bills online, by mail, or in person at the Aurora Bay Utilities Department located within Town Hall.\n",
            "\n",
            "\n",
            "Q2: Where can I pay my water and sewer bills?\n",
            "A2: You can pay water and sewer bills online, by mail, or in person at the Aurora Bay Utilities Department located within Town Hall.\n",
            "\n",
            "\n",
            "Q3: Where can I pay my water and sewer bills?\n",
            "A3: You can pay water and sewer bills online, by mail, or in person at the Aurora Bay Utilities Department located within Town Hall.\n",
            "\n",
            "\n",
            "Ready for your next question!\n",
            "\n",
            "You: exit\n",
            "Thanks for chatting! Have a great day in Aurora Bay\n"
          ]
        }
      ],
      "source": [
        "def start_aurora_bay_chat():\n",
        "    print(\"Hi! I’m an assistant for Aurora Bay, Alaska.\\n\\nWhat would you like to know? (Type 'exit / end' to end)\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_question = input(\"You: \").strip()\n",
        "        if user_question.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Thanks for chatting! Have a great day in Aurora Bay\")\n",
        "            break\n",
        "\n",
        "        # Step 1: Vector Search\n",
        "        faq_matches = search_faq_results(user_question)\n",
        "\n",
        "        # Step 2: Gemini Response\n",
        "        answer = generate_answer_from_gemini(user_question, faq_matches)\n",
        "        print(\"\\n Gemini: \" + answer + \"\\n\")\n",
        "\n",
        "        # Step 3: Ask for resource view or next\n",
        "        next_action = input(\"Would you like to view the sources? Type 'yes' to see FAQs, 'next' to ask another question, or 'exit / end' to quit: \").strip().lower()\n",
        "\n",
        "        if next_action == \"yes\":\n",
        "            print(\"\\nTop Matching FAQs:\")\n",
        "            for idx, row in faq_matches.iterrows():\n",
        "                print(f\"\\nQ{idx+1}: {row['question']}\\nA{idx+1}: {row['answer']}\\n\")\n",
        "\n",
        "        elif next_action == \"exit\" or next_action == 'end' :\n",
        "            print(\"\\nGoodbye! Feel free to come back with more questions.\")\n",
        "            break\n",
        "\n",
        "        print(\"\\nReady for your next question!\\n\")\n",
        "\n",
        "start_aurora_bay_chat()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "student-00-7831d11ee1f7 (Jun 16, 2025, 12:09:23 PM)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
